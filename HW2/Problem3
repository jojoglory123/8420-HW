import numpy as np
from numpy.linalg import eig
from numpy.linalg import inv
import matplotlib.pyplot as plt



##### Function to calculate loss and gadient given betas in each iteration
def loss_and_grad_function(X, Y, Beta, Lambda):
    n = X.shape[0]
    loss = ((X@Beta - Y).T @ (X@Beta - Y))/(2*n) + Beta.T @ Beta * (Lambda/(2*n))
    grad = X.T @ (X@Beta - Y) / n + Beta * (Lambda/n)
    return [loss, grad]

##### Alpha is step size (learning rate), Lambda is strength of regularization
def LinearRegressionGradientDescent(X, Y, Alpha, Lambda, Tol, Iter_Max):
    iter = 0
    beta = np.zeros(X.shape[1]).reshape(-1, 1)
    ##### store initial loss (calculated by initial beta)
    loss_history = loss_and_grad_function(X=X, Y=Y, Beta=beta, Lambda=Lambda)[0]
    ##### creat a list to store |beta_k+1 - beta_*| / |beta_k - beta_*|
    ratio_list = []
    ##### theoretical solution to ridge regression
    beta_star = inv(X.T@X + Lambda*np.identity(X.shape[1])) @ X.T @ Y
    while True:
        ##### use loss_best - loss <tol to set stopping condition
        loss_best = loss_history.min()
        ##### record |beta_k - beta_*|
        before = np.sqrt( (beta - beta_star).T @ (beta - beta_star) ).item()
        ##### update beta
        grad = loss_and_grad_function(X=X, Y=Y, Beta=beta, Lambda=Lambda)[1]
        beta = beta - Alpha * grad
        ##### record |beta_k+1 - beta_*|
        after = np.sqrt( (beta - beta_star).T @ (beta - beta_star)  ).item()
        ##### store ratio
        ratio_list.append(after/before)
        ##### append new loss to loss history
        loss = loss_and_grad_function(X=X, Y=Y, Beta=beta, Lambda=Lambda)[0]
        loss_history = np.append(loss_history, loss)
        iter += 1

        if loss_best - loss < Tol or iter == Iter_Max:
            break

    return [beta, loss_history, iter, ratio_list]

A = np.array([[1, 2, 4], [1, 3, 5], [1, 7, 7], [1, 8, 9]])
y = np.array([[1], [2], [3], [4]])
w,v = eig(A.T @ A)
w_max = w.max()
lamb = 0
alpha = 1/(lamb + w_max)
tol = 10**-10
iter_max = 10**6

[beta, loss_history, iter, ratio_list] = LinearRegressionGradientDescent(X=A, Y=y, Alpha=alpha, Lambda=lamb, Tol=tol, Iter_Max=iter_max)
print("*********************Coefficients from Gradient Descent******************")
print(beta)

print("*************(Verification) Coefficients from Analytical Solution********")
print(inv(A.T@A + lamb*np.identity(A.shape[1])) @ A.T @ y)

plot1 = plt.figure(1)
plt.plot(ratio_list)
plt.axis([0, 55, 0.985, 1.001])
plt.axhline(y = 1, color = 'r', linestyle = '--')
plt.text(25,0.9991,'Bounded by y=1', color = "r", fontsize=14)
plt.ylabel(r"${\||\beta_{k+1} - \beta^*\||_2\quad}/{\quad\||\beta_{k} - \beta^*\||_2}$")
plt.xlabel("Epochs")
plt.title(r"Convergence Rate with $\lambda$=0")


lamb_list = [0.1, 1, 10, 100, 200]
ratio_list_list = []
for i in lamb_list:
    ratio_list = LinearRegressionGradientDescent(X=A, Y=y, Alpha=1/(i + w_max), Lambda=i, Tol=tol, Iter_Max=iter_max)[3]
    ratio_list_list.append(ratio_list)

plot2 = plt.figure(2)
plt.plot(ratio_list_list[0], label=r"$\lambda$ = 0.1")
plt.plot(ratio_list_list[1], label=r"$\lambda$ = 1")
plt.plot(ratio_list_list[2], label=r"$\lambda$ = 10")
plt.plot(ratio_list_list[3], label=r"$\lambda$ = 100")
plt.plot(ratio_list_list[4], label=r"$\lambda$ = 200")
plt.axis([0, 55, 0.74, 1.01])
plt.legend(loc="lower right")
plt.ylabel(r"${\||\beta_{k+1} - \beta^*\||_2\quad}/{\quad\||\beta_{k} - \beta^*\||_2}$")
plt.xlabel("Epochs")
plt.title(r"Convergence Rate with Different $\lambda$'s")
plt.show()
