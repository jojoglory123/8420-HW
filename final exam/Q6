import numpy as np
import matplotlib.pyplot as plt
import time

#########################################################################
##### GD for logistic regression
#########################################################################
np.random.seed(2022)
n = 100
p = 99
X = np.random.rand(n,p)
y = np.random.randint(2, size=n).reshape(-1, 1)

X = np.c_[np.ones(X.shape[0]), X]

def sigmoid(Z):
    result = np.zeros((Z.shape[0], Z.shape[1]))
    for i in range(Z.shape[0]):
        for j in range(Z.shape[1]):
            result[i, j] = 1/(1+np.exp(-Z[i, j]))
    return result

def loss_function(X, y, theta, lamb):
    m = X.shape[0]
    theta_no_intercept = theta[1:, 0].reshape(-1, 1)
    J = ( y.T @ np.log(sigmoid(X@theta)) +  (1-y).T @ np.log(1- sigmoid(X@theta)) )/ (- m)  + theta_no_intercept.T @ theta_no_intercept * (lamb/(2*m))
    grad = X.T @ (sigmoid(X@theta) - y) / m  + np.r_[np.array([[0]]), (lamb/m)*theta_no_intercept]

    return [J, grad]

def LogisticRegressionGradientDescent(X, y, lamb, alpha, tol, iter_max):
    iter = 0
    theta = np.zeros(X.shape[1]).reshape(-1, 1)
    J_history = loss_function(X, y, theta, lamb)[0]
    while True:
        J_best = J_history.min()

        # gradient descent iteration, see Andrew's class for details
        grad = loss_function(X, y, theta, lamb)[1]
        theta = theta - alpha * grad

        J = loss_function(X, y, theta, lamb)[0]
        J_history = np.append(J_history, J)
        iter += 1

        if np.absolute(J_best - J) < tol or iter == iter_max:
            break

    return [theta, J_history, iter]

##### since we don't use regularization here, we set strength lambda = 0
lamb = 0
alpha = 0.02
tol = 10**-4 #alpha = 0.1, tol = 10**-7 you will get Andrew's result
iter_max = 10000

start = time.time()
[theta, J_history, iter] = LogisticRegressionGradientDescent(X, y, lamb, alpha, tol, iter_max)
end = time.time()


##### track loss (J) in iterations
plot1 = plt.figure(1)
plt.plot(J_history*n) #since the function calculates loss, which is average of obj
plt.xlabel("Epochs")
plt.ylabel("Objective Value")
plt.title("GD, n=100, p=99")


print("Epochs to stop:", iter)
print("Execution time:", end - start)


plt.show()





















import numpy as np
import matplotlib.pyplot as plt
from scipy.special import expit
import time

#########################################################################
##### Newton's method for logistic regression
#########################################################################
np.random.seed(2022)
n = 100
p = 99
X = np.random.rand(n,p)
y = np.random.randint(2, size=n)

X = np.c_[np.ones(X.shape[0]), X]
np.place(y, y == 0, -1)

def Grad(X, Y, coef):
    temp = np.zeros(len(coef))
    for i in range(0, X.shape[0]):
        temp = temp + Y[i] * (1 - expit(Y[i] * np.dot(X[i], coef))) * X[i]
    return temp

def Hess(X, Y, weights):
    temp = np.zeros((len(weights), len(weights)))
    for i in range(0, X.shape[0]):
        sig =  expit(np.dot(X[i], weights))
        temp += sig * (1 - sig) * np.outer(X[i], X[i])
    return np.linalg.inv(-temp)

def obj(X, Y, coef):
    temp = 0
    for i in range(0, X.shape[0]):
        temp += -np.log(expit(Y[i] * np.dot(X[i], coef)))
    return temp

tol = 10**-4
iter_max = 10000
iter = 0
w = np.zeros(X.shape[1])
obj_list = []
obj_list.append(obj(X, y, w))

start = time.time()
while True:
    obj_best = min(obj_list)
    learning_rate = 1.0 / (np.sqrt(iter + 1))
    w = w - (learning_rate * np.dot(Hess(X, y, w), Grad(X, y, w)))
    obj_temp = obj(X, y, w)
    obj_list.append(obj_temp)
    iter += 1
    if np.absolute(obj_best - obj_temp) < tol or iter == iter_max:
        break
end = time.time()

plot1 = plt.figure(1)
plt.plot(obj_list)
plt.xlabel("Epochs")
plt.ylabel("Objective Value")
plt.title("Newton's, n=100, p=99")


print("Epochs to stop:", iter)
print("Execution time:", end - start)

plt.show()






















import numpy as np
from sklearn.linear_model import SGDClassifier
import matplotlib.pyplot as plt
from io import StringIO
import sys
import time

#########################################################################
##### SGD for logistic regression
#########################################################################
np.random.seed(2022)
n = 100
p = 99
X = np.random.rand(n,p)
y = np.random.randint(2, size=n)


old_stdout = sys.stdout
sys.stdout = mystdout = StringIO()

start = time.time()
SGDmodel = SGDClassifier(loss="log", tol=10**-4, verbose=1).fit(X, y)
end = time.time()

sys.stdout = old_stdout
loss_history = mystdout.getvalue()
loss_list = []
for line in loss_history.split('\n'):
    if(len(line.split("loss: ")) == 1):
        continue
    loss_list.append(float(line.split("loss: ")[-1]))

plot1 = plt.figure(1)
plt.plot(np.arange(len(loss_list)), loss_list)
plt.xlabel("Epochs")
plt.ylabel("Objective Value")
plt.title("SGD, n=100, p=99")



print("Epochs to stop:", SGDmodel.n_iter_)
print("Execution time:", end - start)

plt.show()
